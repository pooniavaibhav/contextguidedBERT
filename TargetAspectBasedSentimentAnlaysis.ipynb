{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TargetAspectBasedSentimentAnlaysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuJCSkFKQyWPZq8ZRNxuQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pooniavaibhav/contextguidedBERT/blob/main/TargetAspectBasedSentimentAnlaysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfA5w3dpe6Yx"
      },
      "source": [
        "A given text can have different targets (e.g.,neighborhoods) and different aspects (e.g., price or safety),with  different  sentiment  associated  with  each  target-aspectpair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iR_GPbPfvhe"
      },
      "source": [
        "We  investigate  whether  adding  contextto self-attention models improves performance on (T)ABSA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcB4Y7g0gm5Z"
      },
      "source": [
        "# **We  propose  two  variants  of  Context-Guided  BERT-**\n",
        "\n",
        "1. CG-BERT that uses context-guidedsoftmax-attention.\n",
        "2. CG-BERT modelthat  learns  a  compositional  attention  that  supports  subtrac-tive attention.\n",
        "\n",
        "We train both models with pretrained BERT ontwo (T)ABSA datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NFBbT6Kh81u"
      },
      "source": [
        "# Usecase\n",
        "## (ABSA)Aspect based sentiment anlysis\n",
        "People are using online review platforms like swiggy and delivery platforms more frequently,understanding the types of emotional content generated onsuch platforms could yield business insights or provide per-sonalized  recommendations.\n",
        "\n",
        "To this end, Sentiment Analysis techniques have been ap-plied to understand the emotional content generated on microblogs. \n",
        "\n",
        "#Bottleneck-\n",
        "User-generated reviews contain more complex information than just a single overall sentiment.\n",
        "\n",
        "suppose  a user commented.\n",
        "\"Food was tasty but the price was too high\".\n",
        "\n",
        "This sentence contains multiple aspects like tasty and price.\n",
        "\n",
        "In such cases the their associated sentiment has been fomrmalised as a task called **Aspect based sentiment analysis.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExUBGBrMgkOf"
      },
      "source": [
        "## Target based sentiment analysis-\n",
        "Targeted ABSA (TABSA) is a more general version of ABSA, when there are multiple targets in a review, each with their associated aspects.\n",
        "\n",
        "For example, given a review of neighborhoods: “LOC1 area is more expensive but has a better selection of amenities than in LOC2”.We  note  that  the sentiment depends on the specific target (LOC1orLOC2)and their aspect.\n",
        "\n",
        "The sentiment towards the price of LOC1 may be negative—and may be more important to a price-conscious  student—but  positive  in  terms  of convenience,while the sentiment towards LOC2’s aspects are reversed.\n",
        "\n",
        "Research using neural models for (T)ABSA has mainlyfocused  on  using  deep  neural  networks  such  as  RNNs  or attention-gated networks to generate context-dependent sen-tence  embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKbpu3XElcun"
      },
      "source": [
        "## These approaches simply used a pretrainedBERT model as a blackbox: either via using BERT as an em-bedding layer or appending the aspect to the input sentence.\n",
        "\n",
        "We  propose  to  improve  the  BERT  model  architectureto  becontext-aware.A context-aware model\n",
        "should  distribute  its  attention  weights  appropriately under different contexts—in (T)ABSA, this means specifictargets and aspects. Additionally, by incorporating context into  the  calculation  of  attention  weights,  we  aim  to  en-rich the learnt hidden representations of the models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b4qWZK3mgtj"
      },
      "source": [
        "# Specif-ically,  we  propose  two  methods  to  integrate  context  into the  BERT  architecture.\n",
        "\n",
        "\n",
        "1. A  Context-Guided  BERT  (CG-BERT)  model  adapted  from  a  recent  context-aware  self-attention network, which  we  apply  to (T)ABSA; \n",
        "2. Quasi-Attention   Context-GuidedBERT model (QACG-BERT) which enables subtractive attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf9URgAzuHNv"
      },
      "source": [
        "# Formulation of Quasi attention-\n",
        "Attention has been successfully applied to many NLP tasks.Various forms of attention are proposed including additive attention,dot-product attention.\n",
        "Many  of  them  rely  on  a softmax activation  function  to  calculate  attention  weights  for  each  position.as a result the  output  vector  is  in  the  convex  hull  formed  by all other hidden input vectors, preventing the attention gate from learning subtractive relations.\n",
        "\n",
        "To overcome this limitation allow attention weights to be negative (“quasi” attention), which allows input vectors to add to (+1), not contribute to (0), and even subtract from (−1) the output vector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81lLf_SRyutL"
      },
      "source": [
        "## ABSA formulation-\n",
        "self-attention-based  models  such  as  BERT  have been applied to ABSA, by using BERT as the embedding layer or fine-tuning BERT-based models with anABSA classification output layer.\n",
        "\n",
        "## TABSA formulation-\n",
        "We can fine-tune BERT for  TABSA  either  by  (i)  constructing  auxiliary  sentenceswith different pairs of targets and aspects or (ii) modifyingthe top-most classification layer to also take in targets and aspects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnaGuDu4layZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}